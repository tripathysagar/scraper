"""Fill in a module description here"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_playwright_.ipynb.

# %% auto 0
__all__ = ['DEBUG', 'BROWSERS', 'log_err', 'get_brow', 'get_href', 'download_file', 'download_html', 'crawl']

# %% ../nbs/01_playwright_.ipynb 2
import asyncio
from playwright.async_api import async_playwright, Page, Playwright, Browser
import traceback
import re
from fastcore.all import *
from urllib.parse import urlparse, urlencode, quote_plus, unquote
from .core import *
import requests
from collections import deque

# %% ../nbs/01_playwright_.ipynb 3
DEBUG = True
BROWSERS = {
    "ch": lambda pw: pw.chromium,
    "ff": lambda pw: pw.firefox,
    "wk": lambda pw: pw.webkit,
}

async def get_brow(pw: Playwright, brow_n: str):
    """
    return browser_name object from 
    ch -> chromium
    ff -> firefox
    wk -> webkit
    """
    browser_func = BROWSERS.get(brow_n)
    if not browser_func:
        raise ValueError(f"Unknown browser: {brow_n}")
    return await browser_func(pw).launch(headless=DEBUG==False)

# %% ../nbs/01_playwright_.ipynb 5
async def get_href(page:Page):
    """
    Takes in Page object and get back all the href which are not part of `ignore_href`.\n
    It is doen by loop through all the a tags.
    """
    try:
        links = [await tag.get_attribute('href')  for tag in await page.query_selector_all('a')]
        return [ link for link in links if valid_href(link) ]
    except Exception as e:
        print(f"failed for {await page.url}")
        traceback.print_exc()
        raise e

# %% ../nbs/01_playwright_.ipynb 10
log_err = lambda func, url, err: print(f"Error in {func=}\n{url=}\n{err}")

def download_file(url:str, fn:Path='.'):
    """
    Downloads a file from the specified URL and saves it to the given path.
    Args:
        url (str): The URL from which to download the file.
        fn (Path): The destination file path to save the downloaded content.
    Raises:
        Exception: If an error occurs during the download or saving process.
    """
    try:
        resp = requests.get(url)
        if resp.status_code == 200:
            with open(fn, 'wb') as f:
                f.write(resp.content)
    except Exception as e:
        log_err('download_file', url, e)
        traceback.print_exc()
Path('Test/dummy.pdf').unlink(missing_ok=True)

# %% ../nbs/01_playwright_.ipynb 13
async def download_html(brow:Browser, url:str, fn:Path):
        """
        takes in url either ending with .html or any abs adress
        retruns all the href with the abs links
        """
        
        page = await brow.new_page()

        try:
            await page.goto(url) 
            #c_typ =  resp.headers.get('content-type', '').lower()
            #c_typ_ext = [k for k, v in ALLOWED_EXT_CONTENT_TYPS.items() if v in c_typ][0][1:]
            #print(f"{url=}\n{fn=}\n{f_ext=}\n{c_typ=}\n{c_typ_ext=}")
            
            with open(fn, 'w', encoding='utf-8') as f:
                    f.write(await page.content())
            return await get_href(page)
 
        except Exception as e:
            log_err('download_html', url, e)
            traceback.print_exc()
        finally:
            await page.close()
        return []

# %% ../nbs/01_playwright_.ipynb 15
import traceback

async def crawl(url: str, dir_n: Path, brow_typ: str = "ch"):
    """
    Asynchronously crawls and downloads HTML and specific file types within a given domain.

    Args:
        url (str): The starting URL for crawling.
        dir_n (Path): The directory path where downloaded files will be saved.
        brow_typ (str): The browser type (default is "ch").
    """
    local_domain = urlparse(url).netloc
    queue = deque([url])
    seen = set()

    # Create directory for the domain
    dir_n = dir_n / local_domain
    dir_n.mkdir(parents=True, exist_ok=True)

    async with async_playwright() as pw:
        brow = await get_brow(pw, brow_typ)
        while queue:
            url = queue.pop()
            try:
                if url and url not in seen:
                    #print(f"{url=}")
                    fn = get_fn_from_url(url)
                    f_ext = fn.split('.')[-1]  # Get file extension

                    if f_ext == 'html':
                        links = await download_html(brow, url, dir_n / fn)
                        # Ensure each link is valid for deque.extend
                        queue.extend([hydrate_links(local_domain, i) for i in links])
                    elif '.' + f_ext in ['.pdf', '.doc', '.docx', '.odt', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.csv']:
                        download_file(url, dir_n / fn)
                    else:
                        print(f"Cannot process {url=}")
                seen.add(url)
            except Exception as e:
                print(f"Error processing {url=}:")
                traceback.print_exception(type(e), e, e.__traceback__)

        await brow.close()

