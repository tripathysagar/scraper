{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# req\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from html2text import HTML2Text  \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import traceback\n",
    "from scraper.core import *\n",
    "from lxml.html import fromstring, HtmlElement, tostring\n",
    "from lxml_html_clean import Cleaner\n",
    "\n",
    "h2t = HTML2Text(bodywidth=1000_000)\n",
    "h2t.ignore_links = True\n",
    "h2t.mark_code = True\n",
    "h2t.ignore_images = True\n",
    "cleaner = Cleaner(javascript=True, style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Req:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.fn = get_fn_from_url(url)\n",
    "        self.loc_domain = urlparse(self.url).netloc\n",
    "        self.dir = None\n",
    "\n",
    "    def html2md(self):\n",
    "        try:\n",
    "            resp = requests.get(self.url, timeout=100)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            body = cleaner.clean_html(fromstring(str(soup)).xpath('//body')[0])\n",
    "\n",
    "            assert isinstance(body, HtmlElement)\n",
    "\n",
    "            with open(self.dir/f\"{self.fn.split('.')[0]}.md\", 'w', encoding='utf-8') as f:\n",
    "                    f.write(h2t.handle(tostring(body, encoding='unicode')))\n",
    "\n",
    "            self.links = [link for link in [hydrate_links(self.loc_domain, a['href']) \n",
    "                                            for a in soup.find_all('a', href=True) \n",
    "                                            if valid_href(a['href'])] if link]\n",
    "\n",
    "        except Exception as e:\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)\n",
    "    \n",
    "    def download_file(self):\n",
    "        \"\"\"\n",
    "        Downloads a file from the specified URL and saves it to the given path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            resp = requests.get(self.url)\n",
    "            if resp.status_code == 200:\n",
    "                with open(self.dir/self.fn, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def __call__(self, dir:Path):\n",
    "        self.dir = dir/self.loc_domain\n",
    "        self.dir.mkdir(parents=True, exist_ok=True)\n",
    "        f_ext = '.'+self.fn.split('.')[-1]\n",
    "\n",
    "        if f_ext == '.html':\n",
    "            self.html2md()\n",
    "        elif f_ext in ['.pdf', '.doc', '.docx', '.odt', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.csv']:\n",
    "            self.download_file()\n",
    "        else:\n",
    "            print(f\"could not process {self.url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Req('https://mazagondock.in/')\n",
    "r(Path('./Text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
