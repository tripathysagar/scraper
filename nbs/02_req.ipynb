{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# req\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from html2text import HTML2Text  \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import traceback\n",
    "from scraper.core import *\n",
    "from lxml.html import fromstring, HtmlElement, tostring\n",
    "from lxml_html_clean import Cleaner\n",
    "from collections import deque\n",
    "import sys\n",
    "from fastcore.all import *\n",
    "from time import time\n",
    "\n",
    "h2t = HTML2Text(bodywidth=1000_000)\n",
    "h2t.ignore_links = True\n",
    "h2t.mark_code = True\n",
    "h2t.ignore_images = True\n",
    "cleaner = Cleaner(javascript=True, style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Req:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.fn = get_fn_from_url(url)\n",
    "        self.loc_domain = urlparse(self.url).netloc\n",
    "        self.dir = None\n",
    "\n",
    "    def html2md(self):\n",
    "        try:\n",
    "            resp = requests.get(self.url, timeout=100)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "            body = fromstring(str(soup)).xpath('//body')\n",
    "        \n",
    "            if not body:\n",
    "                raise ValueError(\"No <body> tag found in HTML.\")\n",
    "            \n",
    "            body = cleaner.clean_html(body[0])\n",
    "\n",
    "            assert isinstance(body, HtmlElement)\n",
    "            \"\"\"\n",
    "            print([a.get('href') for a in body.xpath('//a[@href]')])\n",
    "            print([valid_href(a.get('href')) for a in body.xpath('//a[@href]')])\n",
    "\n",
    "            print(self.loc_domain)\n",
    "            print([hydrate_links(self.loc_domain, a.get('href')) \n",
    "                                            for a in body.xpath('//a[@href]') \n",
    "                                             ])\n",
    "            print(f\"{self.fn.split('.html')[0]}.md\")\n",
    "            \n",
    "            \"\"\"\n",
    "            with open(self.dir/f\"{self.fn.split('.html')[0]}.md\", 'w', encoding='utf-8') as f:\n",
    "                    f.write(h2t.handle(tostring(body, encoding='unicode')))\n",
    "            \n",
    "            self.links = set([link for link in [hydrate_links(self.loc_domain, a.get('href')) \n",
    "                                            for a in body.xpath('//a[@href]') \n",
    "                                            if  valid_href(a.get('href')) ] if link])\n",
    "        except Exception as e:\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)\n",
    "    \n",
    "    def download_file(self):\n",
    "        \"\"\"\n",
    "        Downloads a file from the specified URL and saves it to the given path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            resp = requests.get(self.url)\n",
    "            if resp.status_code == 200:\n",
    "                with open(self.dir/self.fn, 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "        except Exception as e:\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)\n",
    "    \n",
    "    def __call__(self, dir_n:Path):\n",
    "        try:\n",
    "            self.dir = dir_n/self.loc_domain\n",
    "            self.dir.mkdir(parents=True, exist_ok=True)\n",
    "            f_ext = '.'+self.fn.split('.')[-1]\n",
    "            \n",
    "            if f_ext == '.html':\n",
    "                self.html2md()\n",
    "            elif f_ext in ['.pdf', '.doc', '.docx', '.odt', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.csv']:\n",
    "                self.download_file()\n",
    "            else:\n",
    "                print(f\"could not process {self.url}\")\n",
    "        except Exception as e:\n",
    "            traceback.print_exception(type(e), e, e.__traceback__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "md_fn = Path('Test/fastcore.fast.ai/_.md')\n",
    "md_fn.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test/fastcore.fast.ai/_.md\n"
     ]
    }
   ],
   "source": [
    "req = Req('https://fastcore.fast.ai/')\n",
    "req(Path('Test/'))\n",
    "print(md_fn)\n",
    "assert md_fn.exists(), \"File not found\"\n",
    "assert md_fn.stat().st_size > 0, \"File is empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pdf_fn = Path('Test/www.w3.org/_wai_er_tests_xhtml_testfiles_resources_pdf_dummy.pdf')\n",
    "pdf_fn.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test/www.w3.org/_wai_er_tests_xhtml_testfiles_resources_pdf_dummy.pdf\n"
     ]
    }
   ],
   "source": [
    "req = Req('https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')\n",
    "req(Path('Test/'))\n",
    "print(pdf_fn)\n",
    "assert pdf_fn.exists(), \"File not found\"\n",
    "assert pdf_fn.stat().st_size > 0, \"File is empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "md_fn.unlink(missing_ok=True)\n",
    "pdf_fn.unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class spider:\n",
    "    \"\"\"\n",
    "    A simple web crawler (spider).\n",
    "    The `spider` class is responsible for visiting URLs, processing them to extract \n",
    "    links, and then continuing the crawl recursively. It keeps track of visited \n",
    "    URLs to avoid revisiting the same page.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    url : str\n",
    "        The starting URL for the crawl.\n",
    "    dir : Path\n",
    "        The directory where downloaded content will be saved (default: 'Test/').\n",
    "    to_visit : deque\n",
    "        A deque (queue) of URLs that need to be visited, initialized with the starting URL.\n",
    "    visited : set\n",
    "        A set of URLs that have already been processed to avoid reprocessing.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    show(url, links_sz):\n",
    "        Prints the current status of the crawl, including the URL being processed, \n",
    "        the number of links found, and the sizes of the `visited` and `to_visit` lists.\n",
    "\n",
    "    process_url(url):\n",
    "        Processes a single URL: fetches content, extracts links, and adds new links \n",
    "        to the queue (`to_visit`).\n",
    "\n",
    "    run():\n",
    "        The main method that runs the spider. It processes URLs in `to_visit` using \n",
    "        the `process_url` method. It processes URLs one by one, and extracts and \n",
    "        follows links found on each page.\n",
    "    \"\"\"\n",
    "    def __init__(self, url:str, dir_n:Path=Path('Test/')):\n",
    "        self.url = url\n",
    "        self.dir = dir_n\n",
    "\n",
    "        self.to_visit = deque([url])\n",
    "        self.visited = set()\n",
    "    \n",
    "    def show(self, url, links_sz):\n",
    "        sys.stdout.write(\n",
    "            f\"\\rProcessed: {url[:50]}... | Links found: {links_sz:<10} | \"\n",
    "            f\"Visited: {len(self.visited):<10} | To visit: {len(self.to_visit):<10} \"\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    def process_url(self, url: str):\n",
    "        \"\"\"Process a single URL: fetch content, extract links, and add to queue.\"\"\"\n",
    "        try:\n",
    "            if url not in self.visited:\n",
    "                self.visited.add(url)  # Mark as visited\n",
    "\n",
    "                # Request and process the page\n",
    "                req = Req(url)\n",
    "                req(self.dir)\n",
    "                links = getattr(req, \"links\", [])\n",
    "                self.to_visit.extend(links)\n",
    "                self.show(url, len(links))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    def run(self):\n",
    "        \"\"\"Main method to run the spider, processing URLs with parallel threads\"\"\"\n",
    "        start = time()\n",
    "        while self.to_visit:\n",
    "            # Get URLs to process in the current batch\n",
    "            self.process_url(self.to_visit.pop())\n",
    "        \n",
    "        print(f\"\\ntime taken: {(time()-start):.2} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%rm -rf Test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: https://fastcore.fast.ai/... | Links found: 19         | Visited: 1          | To visit: 19         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: https://fastcore.fast.ai/docments.html... | Links found: 18         | Visited: 23         | To visit: 168          90        \n",
      "time taken: 2e+01 sec\n"
     ]
    }
   ],
   "source": [
    "s = spider('https://fastcore.fast.ai/', Path('Test/'))\n",
    "s.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert len(Path('Test/fastcore.fast.ai').ls()) <= len(s.visited)\n",
    "assert len(s.to_visit) == 0\n",
    "%rm -rf Test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
